---
title: "搭建自己的ChatGPT系列3：模型精调Finetune"
date: 2023-04-12
draft: true
---

### 开篇

基于 Transformers 架构的大型语言模型 (LLM)，如 GPT、T5 和 BERT，已经在各种自然语言处理 (NLP) 任务中取得了最先进的结果。此外，还开始涉足其他领域，例如计算机视觉 (CV) (VIT、Stable Diffusion、LayoutLM) 和音频 (Whisper、XLS-R)。传统的范式是对通用网络规模数据进行大规模预训练，然后对下游任务进行微调。与使用开箱即用的预训练 LLM (例如，零样本推理) 相比，在下游数据集上微调这些预训练 LLM 会带来巨大的性能提升。

然而，随着模型变得越来越大，在消费级硬件上对模型进行全部参数的微调变得不可行。此外，为每个下游任务独立存储和部署微调模型变得非常昂贵，因为微调模型与原始预训练模型的大小相同。参数高效微调(PEFT) 方法旨在解决这两个问题！

PEFT 方法仅微调少量 (额外) 模型参数，同时冻结预训练 LLM 的大部分参数，从而大大降低了计算和存储成本。这也克服了 灾难性遗忘 1 的问题，这是在 LLM 的全参数微调期间观察到的一种现象。 PEFT 方法也显示出在低数据状态下比微调更好，可以更好地泛化到域外场景。它可以应用于各种模态，例如 图像分类 以及 Stable diffusion dreambooth。

PEFT 方法还有助于提高轻便性，其中用户可以使用 PEFT 方法调整模型，以获得与完全微调的大型检查点相比，大小仅几 MB 的微小检查点。例如， bigscience/mt0-xxl 占用 40GB 的存储空间，全参数微调将导致每个下游数据集有对应 40GB 检查点。而使用 PEFT 方法，每个下游数据集只占用几 MB 的存储空间，同时实现与全参数微调相当的性能。来自 PEFT 方法的少量训练权重被添加到预训练 LLM 顶层。因此，同一个 LLM 可以通过添加小的权重来用于多个任务，而无需替换整个模型。

简而言之，PEFT 方法使您能够获得与全参数微调相当的性能，同时只有少量可训练参数。


快速入门: 轻量化微调 (Parameter Efficient Fine-Tuning，PEFT)
PEFT 是 Hugging Face 的一个新的开源库。使用 PEFT 库，无需微调模型的全部参数，即可高效地将预训练语言模型 (Pre-trained Language Model，PLM) 适配到各种下游应用。PEFT 目前支持以下几种方法:

注意：只有最新的github上的transformers才支持LLaMA模型，所以需要通过pip + git的方式来安装transformers库





## 参数说明

- MICRO_BATCH_SIZE: 参数指定了每个GPU设备上运行的微小批量（mini-batch）大小。这个参数通常是fine-tuning的一个超参数，需要根据硬件和数据集大小进行调整。微小批量是指从整个训练集中取出一小部分样本进行一次前向传播和反向传播的计算。在每个微小批量内，梯度更新是在GPU上进行的，因此微小批量的大小通常是由GPU的内存容量来确定的。MICRO_BATCH_SIZE的值越大，每个微小批量包含的样本数量就越多，因此计算效率更高。但是如果超过了GPU内存的容量，就会出现OOM（out of memory）错误，导致训练无法进行。因此在选择MICRO_BATCH_SIZE时需要平衡计算效率和GPU内存容量之间的权衡。一般来说，MICRO_BATCH_SIZE的值通常在16到32之间，并且需要根据具体的硬件和数据集大小进行调整。
- BATCH_SIZE: 是指在进行模型训练时，每次从训练集中取出的样本数量。这个参数通常也是模型训练的一个超参数。与MICRO_BATCH_SIZE不同，BATCH_SIZE是指一次性从训练集中取出的样本数量，而不是将样本划分为多个微小批量。BATCH_SIZE的值通常会比MICRO_BATCH_SIZE大得多，因为它不仅受到GPU内存容量的限制，还受到CPU和GPU之间的数据传输速度的限制。BATCH_SIZE的值越大，每个训练步骤中包含的样本数量就越多，因此计算效率更高。但是如果BATCH_SIZE设置得太大，就会导致训练过程中内存的占用过高，从而可能会导致OOM错误。一般来说，BATCH_SIZE的值通常在32到128之间，并且需要根据具体的硬件和数据集大小进行调整。
- EPOCHS：表示整个训练数据集被遍历的次数。例如，如果一个数据集有10000个样本，BATCH_SIZE为32，那么一个epoch将会进行10000/32=313次迭代。通常情况下，需要通过多次epoch来不断优化模型的训练效果。
- LEARNING_RATE：表示在每次参数更新时，参数的更新量大小，也就是学习率。学习率决定了模型在每次迭代中参数更新的速度。通常情况下，需要根据具体的任务和模型结构来调整学习率的大小。
- CUTOFF_LEN：表示输入到模型中的文本最大长度。如果输入文本的长度超过了这个阈值，那么将会被截断。这个参数的设置需要根据模型的输入格式和任务需要进行调整。
- LORA_R：表示LORA模型中的reward。reward是指模型为输入生成的文本的质量，也就是模型生成的文本与期望输出的文本之间的相似度。
- LORA_ALPHA：表示LORA模型中的alpha值。alpha值是用来调整reward和KL散度之间的权重的。
- LORA_DROPOUT：表示LORA模型中的dropout值。dropout是一种防止过拟合的技术，它会在训练过程中随机将一些神经元置为0，从而降低神经网络的复杂度。
- VAL_SET_SIZE：表示用于验证的数据集的大小。训练过程中，通常会将一部分数据集留出来用于验证，以便及时监测模型的训练效果。验证集的大小需要根据具体的数据集和任务进行设置。



